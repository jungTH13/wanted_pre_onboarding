{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "592U6lXs3d2t"
      },
      "source": [
        "# Week3_4 Assignment\n",
        "\n",
        "## [BASIC](#Basic) \n",
        "- Encoder & Decoder Layer 코드를 직접 필사하고 각 함수에 주석을 달 수 있다. \n",
        "\n",
        "## [CHALLENGE](#Challenge)\n",
        "- 텐서의 크기(shape)를 계산할 수 있다. \n",
        "\n",
        "## [ADVANCED](#Advanced)\n",
        "- 완성된 transformer 모델의 모든 학습 가능한 파라미터 이름과 크기(shape)를 출력할 수 있다.\n",
        "\n",
        "### Informs\n",
        "이번 과제에서는 \"[Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)\"의 코드를 필사해본다.   \n",
        "\"Annotated Transformer\"는 \"Attention is all you need\" 논문에서 제안한 transformer 모델을 pytorch 라이브러리로 직접 구현한다.   \n",
        "코드 필사를 통해 다음을 배울 수 있다.    \n",
        "- Encoder, Decoder 구조\n",
        "- Attention Mechanism\n",
        "- \"residual connection\", \"layer normalization\" 등의 구조 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoebvnNZ99r-"
      },
      "source": [
        "코드 필사를 시작하기 앞서, transformer 모델의 최종 구조를 살펴보자.    \n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_full.png?raw=true\" width=\"500\" align=\"center\"/>\n",
        "\n",
        "최종 모델은 `EncoderDecoder()` 클래스에 여러 인스턴스를 생성자의 입력 파라미터로 넣어 생성한다.    \n",
        "앞으로 우리는 `EncoderDecoder()` 클래스와 같은 여러 클래스들을 구현하고 연결할 것이다. 따라서 대략적인 클래스간의 관계를 살펴보고 이해한다면 보다 큰 그림을 가지고 코드 필사를 할 수 있을 것이다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DB6cNaXP99sB"
      },
      "source": [
        "Transformer 모델은 크게 4가지 클래스로 구현된다.    \n",
        "- Frame\n",
        "    - frame 역할을 하는 `EncoderDecoder` 클래스\n",
        "- Input Embedding & Encoding\n",
        "    - 입력값을 벡터화하는 `Embeddings`, `PositionalEncoding`\n",
        "- Encoder & Decoder\n",
        "    - 각 6개 layer를 갖고 있는 `Encoder`, `Decoder`\n",
        "    - layer 1층을 구현한 `EncoderLayer`, `DecoderLayer`\n",
        "- Sublayer\n",
        "    - `EncoderLayer`, `DecoderLayer` 내부에서 사용되는 Sublayer 클래스인 `MultiHeadAttiontion`, `PositionwiseFeedForward`\n",
        "    - Sublayer 클래스들을 연결하는 `SublayerConnection`\n",
        "    \n",
        "아래 좌측 도식에서 각 클래스의 색상은 아래 우측 도식(transformer 구조)의 색상과 맵핑되어 있다.    \n",
        "각 클래스의 역할과 클래스 간 연결 관계를 생각하면서 transformer를 코드로 구현해보자.   \n",
        "\n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_map.png?raw=true\" width=\"400\" height=\"400\" align=\"left\"/>\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_transformer.png?raw=true\" width=\"300\" height=\"400\" align=\"right\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaadVYo799sE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import math, copy, time\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OEO0al299sJ"
      },
      "source": [
        "## Basic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKKyKfqB99sL"
      },
      "source": [
        "### Frame\n",
        "- `EncoderDecoder`\n",
        "\n",
        "아래 도식은 `EncoderDecoder` 클래스의 `forward()`, `encode()`, `decode()` 메소드를 도식화 한 것이다.    \n",
        " \n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_encoderdecoder.png?raw=true\" width=500>\n",
        "\n",
        "\n",
        "- `Generator`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MECCTGpt99sP"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "      super(EncoderDecoder,self).__init__()\n",
        "      self.encoder = encoder\n",
        "      self.decoder = decoder\n",
        "      self.src_embed = src_embed\n",
        "      self.tgt_embed = tgt_embed\n",
        "      self.generator = generator\n",
        "    \n",
        "    \n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "      return self.decode(self.encode(src,src_mask),src_mask,tgt,tgt_mask) #encoder에서 출력된 임베딩 벡터를 디코더로 전송\n",
        "    \n",
        "    \n",
        "    def encode(self, src, src_mask):\n",
        "      return self.encoder(self.src_embed(src),src_mask) # 문장을 입력받아 생성된 임베딩벡터와 마스크 정보를 입력\n",
        "    \n",
        "    \n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "      return self.decoder(self.tgt_embed(tgt),memory,src_mask,tgt_mask) # 인코더에서 처리된 정보를 문장2와 마스크정보를 받아 처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Py2wcYPX99sT"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, vocab):\n",
        "      super(Generator,self).__init__()\n",
        "      self.proj=nn.Linear(d_model,vocab) #트랜스포머 디코더 최종단의 단어 선택을 위한 NN모듈을 생성\n",
        "    \n",
        "    \n",
        "    def forward(self, x):\n",
        "      return F.log_softmax(self.proj(x),dim=1) #연산된 정보를 토대로 확율정보 출력"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI-5SRHD99sX"
      },
      "source": [
        "### Encoder\n",
        "- `Encoder`\n",
        "- `EncoderLayer`\n",
        "- `SublayerConnection`\n",
        "- Reference\n",
        "    - Layer Normalization\n",
        "        - [한국어 설명](https://yonghyuc.wordpress.com/2020/03/04/batch-norm-vs-layer-norm/)\n",
        "        - [torch official docs](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)\n",
        "    - Residual Connection\n",
        "        - [한국어 설명](https://itrepo.tistory.com/36)\n",
        "    - pytorch ModuleList\n",
        "        - [torch official docs](https://pytorch.org/docs/1.9.1/generated/torch.nn.ModuleList.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjIjUBjN99sc"
      },
      "outputs": [],
      "source": [
        "def clones(module, N):\n",
        "  return nn.ModuleList([copy.deepcopy(module) for _ in range(N)]) #레이어 층쌓기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgglBAyM99se"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, layer, N):\n",
        "      super(Encoder,self).__init__()\n",
        "      self.layers = clones(layer,N) #입력값에 따른 레이어리스트 생성\n",
        "      self.norm = LayerNorm(layer.size) #정규화를 위한 함수 생성\n",
        "    \n",
        "    \n",
        "    def forward(self, x, mask):\n",
        "      for layer in self.layers: #쌓인 레이어를 통한 연산 실시\n",
        "        x = layer(x,mask)\n",
        "      return self.norm(x) #정규화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvGP8Gsd99sg"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \n",
        "    def __init__(self, features, eps=1e-6):\n",
        "      super(LayerNorm,self).__init__()\n",
        "      self.a_2 = nn.Parameter(torch.ones(features))\n",
        "      self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "      self.eps = eps\n",
        "    \n",
        "    \n",
        "    def forward(self, x):\n",
        "      mean = x.mean(-1,keepdim=True)\n",
        "      std = x.std(-1,keepdim=True)\n",
        "      return self.a_2 * (x - mean) / (std + self.eps)+self.b_2 #평균과 표준편차를 이용한 정규화를 진행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "525_O3YE99si"
      },
      "outputs": [],
      "source": [
        "class SublayerConnection(nn.Module): #잔차연결 및 정규화\n",
        "    \n",
        "    def __init__(self, size, dropout):\n",
        "      super(SublayerConnection,self).__init__()\n",
        "      self.norm = LayerNorm(size) #정규화 클래스\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x, sublayer): # x: 입력 임베딩벡터, sublayer: 서브레이어를 통과한 출력 입베딩벡터\n",
        "      return x + self.dropout(sublayer(self.norm(x))) #출력된 임베딩 벡터를 정규화한 후 잔차연결"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlGCPEVp99sk"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "      super(EncoderLayer,self).__init__()\n",
        "      self.self_attn = self_attn\n",
        "      self.feed_forward = feed_forward\n",
        "      self.sublayer = clones(SublayerConnection(size,dropout),2) #잔차연결 및 정규화를 위한 클래스 생성(MultiHeadedAttention, PositionwiseFeedForward)\n",
        "      self.size = size\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "      x = self.sublayer[0](x, lambda x: self.self_attn(x,x,x,mask)) #하위 레이어 첫번째에 임베딩벡터 x를 전달 (K,Q,V의 3개 벡터를 생성하기 위해 동일값을 3번 전달)\n",
        "      return self.sublayer[1](x,self.feed_forward) #하위 레이어 두번째에 첫번째 하위레이어의 값을 전달하여 신경망연산을 진행"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOiYmYWc99sm"
      },
      "source": [
        "### Decoder\n",
        "- `Decoder`\n",
        "- `DecoderLayer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik47frFO99so"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, layer, N):\n",
        "      super(Decoder,self).__init__()\n",
        "      self.layers = clones(layer,N) #입력값에 따른 레이어리스트 생성\n",
        "      self.norm = LayerNorm(layer.size) #정규화 클래스\n",
        "    \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "      for layer in self.layers: #DecoderLayer로 생성된 레이어를 하나씩 진행하며 연산\n",
        "        x = layer(x,memory,src_mask,tgt_mask) #문자열의 임베딩 벡터를 입력으로 encoder 의 입베딩 벡터를 K,V로 사용하여 어텐션을 진행 \n",
        "      return self.norm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElsG9P7M99sq"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "      super(DecoderLayer, self).__init__()\n",
        "      self.size = size\n",
        "      self.self_attn = self_attn #셀프 어텐션을 진행하는 첫번째 하위 레이어\n",
        "      self.src_attn = src_attn #encoder의 임베딩벡터를 입력받아 어텐션을 진행하는 두번째 하위 레이어\n",
        "      self.feed_forward = feed_forward #신경망 연산을 진행하는 세번째 하위 레이어\n",
        "      self.sublayer = clones(SublayerConnection(size, dropout), 3) #잔차연결 및 정규화를 위한 클래스 생성(MultiHeadedAttention,MultiHeadedAttention, PositionwiseFeedForward)\n",
        "    \n",
        "    \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "      m = memory #encoder에서 연산된 최종 출력값\n",
        "      x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask)) #첫번째 셀프어텐선을 진행(decoder로 입력된 문자열을 마스킹하여 처리)\n",
        "      x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask)) # 이전 하위 레이어의 임베딩벡터를 Q, encoder의 임베딩벡터를 K,V로 하여 두번째 어텐션 진행\n",
        "      return self.sublayer[2](x, self.feed_forward) #세번째 하위레이어로 신경망 연산을 진행"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhPP8LVw99sr"
      },
      "source": [
        "### Sublayer\n",
        "- `attention` 함수\n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_attention.png?raw=true\" width=\"500\" align=\"center\"/>  \n",
        "\n",
        "- `MultiHeadedAttention`\n",
        "- `PositionwiseFeedForward`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o1-iOBu99ss"
      },
      "source": [
        "### Challenge\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0ochH0n99st"
      },
      "source": [
        "### Q1. 위 도식에 따라 `score`, `p_attn`, `attention` 을 구하라 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "ZMYcy8h499sv"
      },
      "outputs": [],
      "source": [
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "  d_k = query.size(-1) #d_model의 사이즈에서 h개의 차원으로 분할되어 각 어텐션의 차원길이를 나타냄\n",
        "  scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) #query,key의 dot연산을 진행후 스케일링(d_k의 루트값으로 다운 스케일)\n",
        "  print('score:',scores.shape)\n",
        "  if mask is not None:\n",
        "      scores = scores.masked_fill(mask == 0, -1e9) #마스킹 존재시 0값을 아주작은 값으로 치환\n",
        "  p_attn = F.softmax(scores, dim = -1) #어텐션 스코어를 통한 softmax의 확율값으로 변환\n",
        "  print('p-attn:',p_attn.shape)\n",
        "  if dropout is not None:\n",
        "      p_attn = dropout(p_attn)\n",
        "  return torch.matmul(p_attn, value), p_attn #어텐션스코어를 반영한 임베딩 벡터 및 어텐션 스코어 반환"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_model=512\n",
        "query, key, value = [torch.ones((12,d_model)) for _ in range(3)]\n",
        "print('shape:',query.shape)\n",
        "print('attention:',attention(query, key, value)[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlxlQ95kx3lF",
        "outputId": "891d1924-bafb-47a8-ec37-6fc290775482"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: torch.Size([12, 512])\n",
            "score: torch.Size([12, 12])\n",
            "p-attn: torch.Size([12, 12])\n",
            "attention: torch.Size([12, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x25aeigL99sw"
      },
      "source": [
        "###Q2. query, key, value가 모두 (m, d_k) shape의 matrix라고 가정할 때, `score`, `p_attn`, `attention`의 shape을 각각 구하라\n",
        "- score : (m, m)\n",
        "- p_attn : (m, m)\n",
        "- attention : (m, d_k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHfxLJKz99sx"
      },
      "source": [
        "### (아래의 **Q3을 먼저 풀고 돌아오세요**) Q4.  query, key, value가 모두 (12, 8, 1, 64) shape의 tensor라고 가정할 때 , `score`, `p_attn`, `attention`의 shape을 각각 구하라\n",
        "\n",
        "- score : (12, 8, 1, 1)\n",
        "- p_attn : (12, 8, 1, 1)\n",
        "- attention : (12, 8, 1, 64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYnffQE799sy"
      },
      "source": [
        "- `MultiHeadedAttention`\n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_multihead.png?raw=true\" width=\"300\" align=\"center\"/>  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhFKlJ2b99sz"
      },
      "outputs": [],
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "      super(MultiHeadedAttention, self).__init__()\n",
        "      assert d_model % h == 0\n",
        "\n",
        "      self.d_k = d_model // h # 임베딩백터의 차원 d_model을 h개로 나누어 h개의 어텐션을 생성했을 때의 각 벡터길이 \n",
        "      self.h = h # 다중처리할 어텐션의 개수\n",
        "      self.linears = clones(nn.Linear(d_model, d_model), 4) #임베딩 벡터를 이용한 Query,Key,Value 생성을 위한 신경망 리스트\n",
        "      self.attn = None\n",
        "      self.dropout = nn.Dropout(p=dropout)\n",
        "    \n",
        "    \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "      if mask is not None:\n",
        "          mask = mask.unsqueeze(1)\n",
        "      nbatches = query.size(0)\n",
        "      \n",
        "      query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) #Q,K,V를 생성, d_model의 길이만큼 생성하여 h개로 나누어 각 어텐션으로 연산\n",
        "                          for l, x in zip(self.linears, (query, key, value))]\n",
        "      \n",
        "      x, self.attn = attention(query, key, value, mask=mask,dropout=self.dropout) # 생성된 Q,K,V를 이용하여 어텐션 수행\n",
        "      \n",
        "      x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
        "      return self.linears[-1](x)\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_model=512\n",
        "h=8\n",
        "query, key, value = [torch.ones((12,d_model)) for _ in range(3)]\n",
        "multiheadedattention=MultiHeadedAttention(h,d_model)\n",
        "nbatches = query.size(0)\n",
        "\n",
        "print('d_k:',multiheadedattention.d_k)\n",
        "print('nn.Linear(d_model, d_model)(query):',nn.Linear(d_model, d_model)(query).shape)\n",
        "print('nn.Linear(d_model, d_model)(query).view(nbatches, -1, h, d_k):',nn.Linear(d_model, d_model)(query).view(nbatches, -1, h, multiheadedattention.d_k).shape)\n",
        "print('nn.Linear(d_model, d_model)(query).view(nbatches, -1, h, d_k).transpose(1,2):',nn.Linear(d_model, d_model)(query).view(nbatches, -1, h, multiheadedattention.d_k).transpose(1,2).shape)"
      ],
      "metadata": {
        "id": "YJxyHzm31ByH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7b92876-4427-41ce-a697-4612b42fc6df"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "d_k: 64\n",
            "nn.Linear(d_model, d_model)(query): torch.Size([12, 512])\n",
            "nn.Linear(d_model, d_model)(query).view(nbatches, -1, h, d_k): torch.Size([12, 1, 8, 64])\n",
            "nn.Linear(d_model, d_model)(query).view(nbatches, -1, h, d_k).transpose(1,2): torch.Size([12, 8, 1, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M46Ensa499s0"
      },
      "source": [
        "### Q3.  query, key, value가 모두 (12, 512) shape의 matrix이고, h 값이 8 이라고 가정할 때, 아래 값의 shape을 각각 구하라\n",
        "\n",
        "- `d_k` (d_k = d_model // h) : 64\n",
        "- `nn.Linear(d_model, d_model)(query)` : (12, 512)\n",
        "- `nn.Linear(d_model, d_model)(query).view(nbatches, -1, h, d_k)` : (12, 1, 8, 64) \n",
        "- `nn.Linear(d_model, d_model)(query).view(nbatches, -1, h, d_k).transpose(1,2)` : (12, 8, 1, 64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twZoeFr799s1"
      },
      "source": [
        "- `PositionwiseFeedForward`\n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_pwff.png?raw=true\" width=\"300\" align=\"center\"/>  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZzpucvQ99s2"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "  def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "      super(PositionwiseFeedForward, self).__init__()\n",
        "      self.w_1 = nn.Linear(d_model, d_ff)\n",
        "      self.w_2 = nn.Linear(d_ff, d_model) #풀리커넥티드 네트워크로 d_ff의 크기의 노드 연산을 거친후 원래 d_model사이즈로 다시 압축\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "      return self.w_2(self.dropout(F.relu(self.w_1(x)))) # d_model -> d_ff -> d_model 사이즈로 노드 연산"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqjsUsbu99s3"
      },
      "source": [
        "### Input Embedding & Encoding\n",
        "- `Embeddings`\n",
        "    - [pytorch official docs](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBVJFurO99s3"
      },
      "outputs": [],
      "source": [
        "class Embeddings(nn.Module):\n",
        "  \n",
        "    def __init__(self, d_model, vocab):\n",
        "      super(Embeddings, self).__init__()\n",
        "      self.lut = nn.Embedding(vocab, d_model) #총 단어 개수의 d_model길이의 임베딩 백터를 생성하는 모듈\n",
        "      self.d_model = d_model #임베딩 벡터 길이\n",
        "  \n",
        "    def forward(self, x):\n",
        "      return self.lut(x) * math.sqrt(self.d_model) #포지션 벡터를 합산시 해당 임베딩벡터의 의미가 희석되는 것을 보안하기 위해 증폭"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po31qs_A99s5"
      },
      "source": [
        "- `PositionalEncoding`\n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_pe.png?raw=true\" width=\"500\" align=\"center\"/>  \n",
        "\n",
        "- `position` 변수 설명\n",
        "    - 모든 position (=최대 토큰 개수)의 값을 갖고 있는 matrix\n",
        "- `div_term` 변수 설명\n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_div.png?raw=true\" width=\"500\" align=\"center\"/>  \n",
        "- `Embedding` + `Encoding` 도식화 \n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_emb_enc.png?raw=true\" width=\"400\" align=\"center\"/>  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RP-_an3x99s5"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, dropout, max_len = 5000):\n",
        "      super(PositionalEncoding, self).__init__()\n",
        "      self.dropout = nn.Dropout(p=dropout)\n",
        "      \n",
        "      \n",
        "      pe = torch.zeros(max_len, d_model)\n",
        "      position = torch.arange(0, max_len).unsqueeze(1)\n",
        "      div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "                            -(math.log(10000.0) / d_model))\n",
        "      pe[:, 0::2] = torch.sin(position * div_term) #cos과 sin을 번갈아 가면서 position matrix를 생성\n",
        "      pe[:, 1::2] = torch.cos(position * div_term)\n",
        "      pe = pe.unsqueeze(0)\n",
        "      self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "      x = x + Variable(self.pe[:, :x.size(1)], #역전파 되지 않도록 설정 후 임베딩 벡터와 합산\n",
        "                        requires_grad=False)\n",
        "      return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNf13Gkm99s6"
      },
      "source": [
        "### Q4.  max_len이 512이고, d_model이 512라고 가정할 때, `position`과 `div_term`의 shape을 구하라\n",
        "\n",
        "- `position` : (512,1)\n",
        "- `div_term` : (256,)\n",
        "- `position * div_term` : (512,256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rri-daP399s7"
      },
      "source": [
        "### Advanced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3ZixTN199s8"
      },
      "source": [
        "### Finally Build Model\n",
        "- Xavier Initialization\n",
        "    - [한국어 자료](https://huangdi.tistory.com/8)\n",
        "    - [pytorch official docs](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_uniform_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPdGsCiC99s8"
      },
      "outputs": [],
      "source": [
        "def make_model(src_vocab, tgt_vocab, \n",
        "               N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "  c = copy.deepcopy\n",
        "  attn = MultiHeadedAttention(h, d_model) # 하위 레이어 어텐션레이어 생성\n",
        "  ff = PositionwiseFeedForward(d_model, d_ff, dropout) #하위 레이어 FFNN 레이어 생성\n",
        "  position = PositionalEncoding(d_model, dropout) #포지션 정보를 생성\n",
        "  model = EncoderDecoder(\n",
        "      Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "      Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
        "                            c(ff), dropout), N),\n",
        "      nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
        "      nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
        "      Generator(d_model, tgt_vocab))\n",
        "  \n",
        "  for p in model.parameters(): #모델의 모든 파라미터값 초기화\n",
        "      if p.dim() > 1:\n",
        "          nn.init.xavier_uniform(p)\n",
        "  return model\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIDN1DSd99s-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2607e90-926c-4141-cb4f-ae876a2333f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
          ]
        }
      ],
      "source": [
        "model = make_model(10,10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljRK80Lo99s_"
      },
      "source": [
        "### Q5. 위 코드로 만든 모델의 모든 파라미터의 이름과 크기 (shape) 을 출력하라"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHubCUOh99tA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dea79d4a-f87c-47c2-f60c-f270d80db023"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "parameter name: encoder.layers.0.self_attn.linears.0.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: encoder.layers.0.self_attn.linears.0.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.0.self_attn.linears.1.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: encoder.layers.0.self_attn.linears.1.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.0.self_attn.linears.2.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: encoder.layers.0.self_attn.linears.2.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.0.self_attn.linears.3.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: encoder.layers.0.self_attn.linears.3.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.0.feed_forward.w_1.weight\n",
            "shape: torch.Size([2048, 512])\n",
            "parameter name: encoder.layers.0.feed_forward.w_1.bias\n",
            "shape: torch.Size([2048])\n",
            "parameter name: encoder.layers.0.feed_forward.w_2.weight\n",
            "shape: torch.Size([512, 2048])\n",
            "parameter name: encoder.layers.0.feed_forward.w_2.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.0.sublayer.0.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.0.sublayer.0.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.0.sublayer.1.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.0.sublayer.1.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.1.self_attn.linears.0.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: encoder.layers.1.self_attn.linears.0.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.1.self_attn.linears.1.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: encoder.layers.1.self_attn.linears.1.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.1.self_attn.linears.2.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: encoder.layers.1.self_attn.linears.2.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.1.self_attn.linears.3.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: encoder.layers.1.self_attn.linears.3.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.1.feed_forward.w_1.weight\n",
            "shape: torch.Size([2048, 512])\n",
            "parameter name: encoder.layers.1.feed_forward.w_1.bias\n",
            "shape: torch.Size([2048])\n",
            "parameter name: encoder.layers.1.feed_forward.w_2.weight\n",
            "shape: torch.Size([512, 2048])\n",
            "parameter name: encoder.layers.1.feed_forward.w_2.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.1.sublayer.0.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.1.sublayer.0.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.1.sublayer.1.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.1.sublayer.1.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.2.self_attn.linears.0.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: encoder.layers.2.self_attn.linears.0.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.2.self_attn.linears.1.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: encoder.layers.2.self_attn.linears.1.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.2.self_attn.linears.2.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: encoder.layers.2.self_attn.linears.2.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.2.self_attn.linears.3.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: encoder.layers.2.self_attn.linears.3.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.2.feed_forward.w_1.weight\n",
            "shape: torch.Size([2048, 512])\n",
            "parameter name: encoder.layers.2.feed_forward.w_1.bias\n",
            "shape: torch.Size([2048])\n",
            "parameter name: encoder.layers.2.feed_forward.w_2.weight\n",
            "shape: torch.Size([512, 2048])\n",
            "parameter name: encoder.layers.2.feed_forward.w_2.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.2.sublayer.0.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.2.sublayer.0.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.2.sublayer.1.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.2.sublayer.1.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.3.self_attn.linears.0.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: encoder.layers.3.self_attn.linears.0.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.3.self_attn.linears.1.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: encoder.layers.3.self_attn.linears.1.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.3.self_attn.linears.2.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: encoder.layers.3.self_attn.linears.2.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.3.self_attn.linears.3.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: encoder.layers.3.self_attn.linears.3.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.3.feed_forward.w_1.weight\n",
            "shape: torch.Size([2048, 512])\n",
            "parameter name: encoder.layers.3.feed_forward.w_1.bias\n",
            "shape: torch.Size([2048])\n",
            "parameter name: encoder.layers.3.feed_forward.w_2.weight\n",
            "shape: torch.Size([512, 2048])\n",
            "parameter name: encoder.layers.3.feed_forward.w_2.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.3.sublayer.0.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.3.sublayer.0.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.3.sublayer.1.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.3.sublayer.1.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.4.self_attn.linears.0.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: encoder.layers.4.self_attn.linears.0.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.4.self_attn.linears.1.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: encoder.layers.4.self_attn.linears.1.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.4.self_attn.linears.2.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: encoder.layers.4.self_attn.linears.2.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.4.self_attn.linears.3.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: encoder.layers.4.self_attn.linears.3.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.4.feed_forward.w_1.weight\n",
            "shape: torch.Size([2048, 512])\n",
            "parameter name: encoder.layers.4.feed_forward.w_1.bias\n",
            "shape: torch.Size([2048])\n",
            "parameter name: encoder.layers.4.feed_forward.w_2.weight\n",
            "shape: torch.Size([512, 2048])\n",
            "parameter name: encoder.layers.4.feed_forward.w_2.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.4.sublayer.0.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.4.sublayer.0.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.4.sublayer.1.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.4.sublayer.1.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.5.self_attn.linears.0.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: encoder.layers.5.self_attn.linears.0.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.5.self_attn.linears.1.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: encoder.layers.5.self_attn.linears.1.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.5.self_attn.linears.2.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: encoder.layers.5.self_attn.linears.2.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.5.self_attn.linears.3.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: encoder.layers.5.self_attn.linears.3.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.5.feed_forward.w_1.weight\n",
            "shape: torch.Size([2048, 512])\n",
            "parameter name: encoder.layers.5.feed_forward.w_1.bias\n",
            "shape: torch.Size([2048])\n",
            "parameter name: encoder.layers.5.feed_forward.w_2.weight\n",
            "shape: torch.Size([512, 2048])\n",
            "parameter name: encoder.layers.5.feed_forward.w_2.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.5.sublayer.0.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.5.sublayer.0.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.5.sublayer.1.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.layers.5.sublayer.1.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: encoder.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.0.self_attn.linears.0.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.0.self_attn.linears.0.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.0.self_attn.linears.1.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.0.self_attn.linears.1.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.0.self_attn.linears.2.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.0.self_attn.linears.2.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.0.self_attn.linears.3.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.0.self_attn.linears.3.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.0.src_attn.linears.0.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.0.src_attn.linears.0.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.0.src_attn.linears.1.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.0.src_attn.linears.1.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.0.src_attn.linears.2.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.0.src_attn.linears.2.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.0.src_attn.linears.3.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.0.src_attn.linears.3.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.0.feed_forward.w_1.weight\n",
            "shape: torch.Size([2048, 512])\n",
            "parameter name: decoder.layers.0.feed_forward.w_1.bias\n",
            "shape: torch.Size([2048])\n",
            "parameter name: decoder.layers.0.feed_forward.w_2.weight\n",
            "shape: torch.Size([512, 2048])\n",
            "parameter name: decoder.layers.0.feed_forward.w_2.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.0.sublayer.0.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.0.sublayer.0.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.0.sublayer.1.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.0.sublayer.1.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.0.sublayer.2.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.0.sublayer.2.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.1.self_attn.linears.0.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.1.self_attn.linears.0.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.1.self_attn.linears.1.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.1.self_attn.linears.1.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.1.self_attn.linears.2.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.1.self_attn.linears.2.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.1.self_attn.linears.3.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.1.self_attn.linears.3.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.1.src_attn.linears.0.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.1.src_attn.linears.0.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.1.src_attn.linears.1.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.1.src_attn.linears.1.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.1.src_attn.linears.2.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.1.src_attn.linears.2.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.1.src_attn.linears.3.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.1.src_attn.linears.3.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.1.feed_forward.w_1.weight\n",
            "shape: torch.Size([2048, 512])\n",
            "parameter name: decoder.layers.1.feed_forward.w_1.bias\n",
            "shape: torch.Size([2048])\n",
            "parameter name: decoder.layers.1.feed_forward.w_2.weight\n",
            "shape: torch.Size([512, 2048])\n",
            "parameter name: decoder.layers.1.feed_forward.w_2.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.1.sublayer.0.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.1.sublayer.0.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.1.sublayer.1.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.1.sublayer.1.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.1.sublayer.2.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.1.sublayer.2.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.2.self_attn.linears.0.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.2.self_attn.linears.0.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.2.self_attn.linears.1.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.2.self_attn.linears.1.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.2.self_attn.linears.2.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.2.self_attn.linears.2.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.2.self_attn.linears.3.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.2.self_attn.linears.3.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.2.src_attn.linears.0.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.2.src_attn.linears.0.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.2.src_attn.linears.1.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.2.src_attn.linears.1.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.2.src_attn.linears.2.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.2.src_attn.linears.2.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.2.src_attn.linears.3.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.2.src_attn.linears.3.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.2.feed_forward.w_1.weight\n",
            "shape: torch.Size([2048, 512])\n",
            "parameter name: decoder.layers.2.feed_forward.w_1.bias\n",
            "shape: torch.Size([2048])\n",
            "parameter name: decoder.layers.2.feed_forward.w_2.weight\n",
            "shape: torch.Size([512, 2048])\n",
            "parameter name: decoder.layers.2.feed_forward.w_2.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.2.sublayer.0.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.2.sublayer.0.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.2.sublayer.1.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.2.sublayer.1.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.2.sublayer.2.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.2.sublayer.2.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.3.self_attn.linears.0.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.3.self_attn.linears.0.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.3.self_attn.linears.1.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.3.self_attn.linears.1.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.3.self_attn.linears.2.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.3.self_attn.linears.2.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.3.self_attn.linears.3.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.3.self_attn.linears.3.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.3.src_attn.linears.0.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.3.src_attn.linears.0.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.3.src_attn.linears.1.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.3.src_attn.linears.1.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.3.src_attn.linears.2.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.3.src_attn.linears.2.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.3.src_attn.linears.3.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.3.src_attn.linears.3.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.3.feed_forward.w_1.weight\n",
            "shape: torch.Size([2048, 512])\n",
            "parameter name: decoder.layers.3.feed_forward.w_1.bias\n",
            "shape: torch.Size([2048])\n",
            "parameter name: decoder.layers.3.feed_forward.w_2.weight\n",
            "shape: torch.Size([512, 2048])\n",
            "parameter name: decoder.layers.3.feed_forward.w_2.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.3.sublayer.0.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.3.sublayer.0.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.3.sublayer.1.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.3.sublayer.1.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.3.sublayer.2.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.3.sublayer.2.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.4.self_attn.linears.0.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.4.self_attn.linears.0.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.4.self_attn.linears.1.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.4.self_attn.linears.1.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.4.self_attn.linears.2.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.4.self_attn.linears.2.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.4.self_attn.linears.3.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.4.self_attn.linears.3.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.4.src_attn.linears.0.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.4.src_attn.linears.0.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.4.src_attn.linears.1.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.4.src_attn.linears.1.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.4.src_attn.linears.2.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.4.src_attn.linears.2.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.4.src_attn.linears.3.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.4.src_attn.linears.3.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.4.feed_forward.w_1.weight\n",
            "shape: torch.Size([2048, 512])\n",
            "parameter name: decoder.layers.4.feed_forward.w_1.bias\n",
            "shape: torch.Size([2048])\n",
            "parameter name: decoder.layers.4.feed_forward.w_2.weight\n",
            "shape: torch.Size([512, 2048])\n",
            "parameter name: decoder.layers.4.feed_forward.w_2.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.4.sublayer.0.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.4.sublayer.0.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.4.sublayer.1.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.4.sublayer.1.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.4.sublayer.2.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.4.sublayer.2.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.5.self_attn.linears.0.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.5.self_attn.linears.0.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.5.self_attn.linears.1.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.5.self_attn.linears.1.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.5.self_attn.linears.2.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.5.self_attn.linears.2.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.5.self_attn.linears.3.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.5.self_attn.linears.3.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.5.src_attn.linears.0.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.5.src_attn.linears.0.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.5.src_attn.linears.1.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.5.src_attn.linears.1.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.5.src_attn.linears.2.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.5.src_attn.linears.2.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.5.src_attn.linears.3.weight\n",
            "shape: torch.Size([512, 512])\n",
            "parameter name: decoder.layers.5.src_attn.linears.3.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.5.feed_forward.w_1.weight\n",
            "shape: torch.Size([2048, 512])\n",
            "parameter name: decoder.layers.5.feed_forward.w_1.bias\n",
            "shape: torch.Size([2048])\n",
            "parameter name: decoder.layers.5.feed_forward.w_2.weight\n",
            "shape: torch.Size([512, 2048])\n",
            "parameter name: decoder.layers.5.feed_forward.w_2.bias\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.5.sublayer.0.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.5.sublayer.0.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.5.sublayer.1.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.5.sublayer.1.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.5.sublayer.2.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.layers.5.sublayer.2.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.norm.a_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: decoder.norm.b_2\n",
            "shape: torch.Size([512])\n",
            "parameter name: src_embed.0.lut.weight\n",
            "shape: torch.Size([10, 512])\n",
            "parameter name: tgt_embed.0.lut.weight\n",
            "shape: torch.Size([10, 512])\n",
            "parameter name: generator.proj.weight\n",
            "shape: torch.Size([10, 512])\n",
            "parameter name: generator.proj.bias\n",
            "shape: torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "# 구현\n",
        "\n",
        "for param in model.named_parameters():\n",
        "  print('parameter name:',param[0])\n",
        "  print('shape:',param[1].shape)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Week3_4_assginment.ipynb의 사본",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "torch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}