{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled147.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "36gnpegtPGMz"
      },
      "outputs": [],
      "source": [
        "class Tokenizer():\n",
        "  def __init__(self):\n",
        "    self.word_dict = {'oov': 0}\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def preprocessing(self, sequences):\n",
        "    result = []\n",
        "    #문제 1-1.\n",
        "    for s in sequences:\n",
        "      s=''.join(c for c in s if str.isalnum(c) or c==' ')\n",
        "      result.append(s.lower().split())\n",
        "    #end\n",
        "    return result\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    self.fit_checker = False\n",
        "    #문제 1-2.\n",
        "    tokens=self.preprocessing(sequences)\n",
        "    num=max(self.word_dict.values())+1\n",
        "    for t in tokens:\n",
        "      for s in t:\n",
        "        if s in self.word_dict:\n",
        "          continue\n",
        "        else:\n",
        "          self.word_dict[s]=num\n",
        "          num+=1\n",
        "    #end\n",
        "    self.fit_checker = True\n",
        "  \n",
        "  def transform(self, sequences):\n",
        "    result = []\n",
        "    tokens = self.preprocessing(sequences)\n",
        "\n",
        "    if self.fit_checker:\n",
        "      #문제 1-3.\n",
        "      for t in tokens:\n",
        "        t_tokens=[]\n",
        "        for s in t:\n",
        "          tw = self.word_dict.get(s)\n",
        "          if not tw:\n",
        "            tw=self.word_dict['oov']\n",
        "          t_tokens.append(tw)\n",
        "        result.append(t_tokens)\n",
        "      #end\n",
        "      return result\n",
        "    else:\n",
        "      raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
        "      \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    result = self.transform(sequences)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from math import log\n",
        "\n",
        "class TfidfVectorizer:\n",
        "  def __init__(self, tokenizer):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    tokenized = self.tokenizer.fit_transform(sequences)\n",
        "    #문제 2-1.\n",
        "    self.IDF=[]\n",
        "    self.tokens=list(self.tokenizer.word_dict.values())\n",
        "    self.tokens.remove(0)\n",
        "    self.tokens_name=list(self.tokenizer.word_dict.keys())\n",
        "    self.tokens_name.remove('oov')\n",
        "    for t in self.tokens:\n",
        "      df=0\n",
        "      for d in tokenized:\n",
        "        df += t in d\n",
        "      self.IDF.append(log(len(sequences)/(1+df)))\n",
        "    #end\n",
        "    self.fit_checker = True\n",
        "    \n",
        "\n",
        "  def transform(self, sequences):\n",
        "    if self.fit_checker:\n",
        "      tokenized = self.tokenizer.transform(sequences)\n",
        "      #문제 2-2.\n",
        "      self.tfidf_matrix=[]\n",
        "      self.tf_matrix=[]\n",
        "      for d in tokenized:\n",
        "        result1=[]\n",
        "        result2=[]\n",
        "        for t,idf in zip(self.tokens,self.IDF):\n",
        "          result1.append(d.count(t)*idf)\n",
        "          result2.append(d.count(t))\n",
        "        self.tfidf_matrix.append(result1)\n",
        "        self.tf_matrix.append(result2)\n",
        "      #end\n",
        "      return self.tfidf_matrix\n",
        "    else:\n",
        "      raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
        "\n",
        "  \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    return self.transform(sequences)"
      ],
      "metadata": {
        "id": "8KsT0Twqbz2f"
      },
      "execution_count": 141,
      "outputs": []
    }
  ]
}